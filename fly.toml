# fly.toml app configuration file generated for ollama-webui-demo on 2024-02-06T11:03:35+02:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = ''               # change this before deploying!
primary_region = 'iad'

[env]
DEFAULT_MODEL = 'llama3.1:8b'
OLLAMA_BASE_URL = 'http://localhost:11434'
OLLAMA_MODELS = '/app/backend/data/models'
ENABLE_SIGNUP = "false"

[[mounts]]
source = 'data'
destination = '/app/backend/data'
initial_size = '20gb'

[http_service]
internal_port = 8080
force_https = true
auto_stop_machines = true
auto_start_machines = true
min_machines_running = 0
processes = ['app']

[[vm]]
size = "performance-8x"
memory = "16gb"
gpu_kind = "a100-80gb"
